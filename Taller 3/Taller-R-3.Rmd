---
title: 'CACCS: Secuencia de taller de RStudio -- Parte 3'
author: "Rashid C.J. Marcano Rivera"
date: "10 de oct.·µâ de 2025"
output:
  html_document:
    theme: cerulean
    toc: TRUE
    toc_float: TRUE
    number_sections: TRUE
  pdf_document:
    latex_engine: xelatex
    toc: true
editor_options: 
  markdown: 
    wrap: 90
always_allow_html: true
---

**Inferencia estad√≠stica**

Este taller est√° basado en elementos y ejemplos del libro de Rafael Irizarry,
[aqu√≠](https://rafalab.dfci.harvard.edu/dslibro), as√≠ como pedazos del curso de la
Universidad de la Rep√∫blica, en Uruguay, disponible en esta p√°gina de [RPubs de
RStudio](https://rpubs.com/lercy/930251) y este repaso sobre modelos longitudinales por
Alessio Crippa tambi√©n en [RPubs de
RStudio](https://rpubs.com/alecri/review_longitudinal). Recomiendo complementar el
an√°lisis con el libro *Data Analysis Using Regression and Multilevel/Hierarchical Models*
de Andrew Gelman y Jennifer Hill.

*Si a√∫n no has instalado R, est√° [aqu√≠](http://cran.us.r-project.org/). Acto seguido,
[baja RStudio](https://posit.co/download/rstudio-desktop/). Puedes tambi√©n ir a la nube
[en Posit Cloud](https://posit.cloud/).*

# Recapitulando

La vez anterior, tomamos un recorrido a trav√©s de distintos tipos de visualizaciones. En
efecto, una buena visualizaci√≥n puede demostrar bastante sobre elementos en nuestros
estudios. Por ejemplo

```{r Cargando datos de Wooldridge}
library(wooldridge)

data(wage1)
head(wage1)

```

¬øQu√© aprendemos de ver estos datos as√≠? ¬øPodemos r√°pidamente determinar a si a√±os de
educaci√≥n se traducen a mayores ingresos? ¬øPodemos determinar si afecta, en algo, la
relaci√≥n marital? Para muchos humanos, es dif√≠cil extraer informaci√≥n con meramente mirar
a n√∫meros sin contexto adicional. Pero podr√≠amos ver algo en este gr√°fico

![Meta 1: datos de salario, educaci√≥n, y estado marital.](RelacioÃÅn.png)

Vivimos en una era de creciente disponibilidad de conjuntos de datos informativos y de
herramientas de software, con lo cual el uso de visualizaciones ha aumentado en diversos
espacios: acad√©micos, gubernamentales, organizaciones sociales, prensa, e industrias
varias. Sin embargo, R, un programa estad√≠stico dise√±ado para el manejo y an√°lisis de
distintos formatos de datos, tenemos un sinn√∫mero de opciones para trabajar distintos
tipos de complejidades en datos y an√°lisis.

En esta secuencia de cuatro semanas, continuamos de lo aprendido en el pasado; aplicaremos
funciones para cargar datos, manipularlos, y retomamos problemas que visualizamos la
semana pasada como el se√±alado, para analizarlos con an√°lisis estad√≠sticos y
visualizaciones relativas a la inferencia estad√≠stica. De una vez estaremos entrando en
algunos de los diagn√≥sticos que podremos desplegar para evaluar qu√© tan adecuado es un
modelo para trabajar ciertos datos. Aprenderemos hoy:

1.  Varias operaciones estad√≠sticas,

2.  Estad√≠stica inferencial, en sus varias versiones para modelos simples, lineales,
    jer√°rquicos y longitudinales.

3.  Gr√°ficos y pruebas que ayuden a entender y diagnosticar los modelos estad√≠sticos que
    usaremos.

# Modelos estad√≠sticos

Quer√≠amos entender la vez anterior la relaci√≥n de ingreso con otras variables. Para cargar
los datos escribiremos

```{r}
library(wooldridge)
base <- wage1
#View(base) 
names(base)
#?wage1
```

Las variables que utilizaremos son las siguientes:

-   wage: salario promedio por hora.

-   educ: a√±os de educaci√≥n.

-   exper: a√±os de experiencia potencial.

-   tenure: a√±os con el empleador actual (antig√ºedad).

-   nonwhite: es igual a 1 si la persona no es blanca, 0 si no.

-   female: es igual a 1 si la persona es mujer, 0 si no

-   married: es igual a 1 si la persona es casada, 0 si no.

En primer lugar queremos cambiar el nombre de la variable que est√° en la posici√≥n 4:

```{r}
names(base)[4] <- "antig√ºedad"
names(base)[24] <- "antig√ºedadcuad"

```

Por ahora lo que nos interesaba es un subconjunto de variables, todas de la 1 a la 7 (la
4.¬™ ha quedado como antig√ºedad), la 22 (log. de salario), la 23 (experiencia al cuadrado)
y la 24 (la antig√ºedad al cuadrado).

```{r, Selecci√≥n de variables}
base1 <- base[,c(1: 7, 22:24)] 
```

Veremos ahora las primeras filas

```{r}
head(base1, n = 10)
```

Tambi√©n podr√≠amos llamarlos con el nombre de variable

```{r, subconjuntos b√°sicos}
datos1 <- base[,c("wage","educ","exper", "antig√ºedad" )] 

head(datos1, n = 5)
```

o como cubrimos al tocar `tidyverse`, la funci√≥n `select()`

```{r, Select()}
library(tidyverse)
colnames(base)
datos2 <- base |> 
  dplyr::select(wage, educ, exper, antig√ºedad) #¬øqu√© pasa si no tengo la especificaci√≥n?
head(datos2, n=7)


```

Si solamente quisi√©ramos los datos de los casados podr√≠amos usar la funci√≥n de filtrado

```{r, S√≥lo casados}
datos3 <- base |> 
  dplyr::select("wage", "educ", "exper", "antig√ºedad", "married") |> 
  filter(married == 1)
```

# Correlaciones

Para investigar si hay correlaci√≥n entre alguna de las variables se puede realizar un
gr√°fico en el que se presenta la dispersi√≥n para cada par de variables.

```{r, Correlaciones}
plot(datos2)
```

Y tambi√©n calcular la matriz de correlaciones de las variables que figuran en *datos2*.

```{r, Correlaciones 2}
cor(datos2)
```

Notamos que existe una correlaci√≥n positiva entre el salario y la educaci√≥n (0.4059)

Crearemos el diagrama de dispersi√≥n entre salario y educaci√≥n utilizando las funciones de
la librer√≠a ggplot2.

```{r}
ggplot(datos2, aes(x = educ, y = wage)) + 
  geom_point() + theme_light() +
  ggtitle("Relaci√≥n entre salario y educaci√≥n")
```

Se acordar√°n que la vez anterior a√±adimos complicaciones como un nivel adicional en la
capa de color:

```{r}
wage1 |> 
  mutate(marital = factor(married, levels = c(0, 1), labels = c("Soltero", "Casado"))) |> 
  ggplot(aes(educ, wage)) +  geom_point(aes(colour = marital), size = 3)+
  labs(title="Relaci√≥n entre educaci√≥n y salario",
       x = "Educaci√≥n", 
       y = "Salario",
       color = "Estado marital",
       caption = "Datos de Wooldridge, usado en p√°gs. como 7, 17, 33-34, 37, 76,...")

```

Esto lo exploraremos m√°s al entrar en modelos de regresi√≥n lineal *m√∫ltiple*. Empezaremos
por el modelo de regresi√≥n lineal *simple*.

# Modelo de regresi√≥n lineal simple

Estimamos un modelo de regresi√≥n lineal simple, con el m√©todo m√≠nimos cuadrados ordinarios
(en adelante MCO, OLS en ingl√©s) que explique los salarios en funci√≥n de los a√±os de
educaci√≥n de las personas. En R esto se hace con la funci√≥n `lm()`:

```{r, MCO}
mod1 <- lm(wage ~ educ, data=datos2)
summary(mod1) # para imprimir la salida
```

¬øC√≥mo se lee esta salida? La primera l√≠nea indica la f√≥rmula que se utiliz√≥. La segunda es
sobre la distribuci√≥n de residuos las diferencias entre los valores observados de `wage` y
los valores predichos por el modelo. La mediana cercana a cero indica que los residuos
est√°n centrados alrededor de cero. El rango de los residuos sugiere que hay algunos
valores at√≠picos, especialmente en el extremo m√°ximo (16.6085), lo que podr√≠a indicar la
presencia de salarios excepcionalmente altos no explicados completamente por el modelo.

Vamos a los coeficientes: la informaci√≥n se presenta a trav√©s de varias columnas. La
columna de `Estimate` tiene el valor estimado de coeficientes ($\hat{\beta}$, mientras que
la columna de `Std. Error` nos da en promedio lo que var√≠a el estimado en relaci√≥n a la
variable dependiente. Esto es de utilidad para computar intervalos de confianza y
establecer la m√©trica con la cual determinar la hip√≥tesis la existencia de una relaci√≥n
entre una variable y otra. El puntaje t reporta la distancia estandarizada en distribuci√≥n
t de nuestro coeficiente, en relaci√≥n a la posibilidad de que tuviera cero efecto.
Mientras mayor sea el n√∫mero de puntaje, y se mantuvieran relativamente mayores en
relaci√≥n al error est√°ndar indica que una relaci√≥n existe. La √∫ltima columna provee el
valor p, probabilidad asociada al puntaje t, que indica la probabilidad de estar viendo un
valor tan extremo como el reportado por azar.

El intercepto de -0.9 representa el salario promedio cuando los a√±os de educaci√≥n son
cero. Sin embargo, en la pr√°ctica, es poco com√∫n que una persona tenga cero a√±os de
educaci√≥n (o que tenga ingreso negativo por un trabajo), por lo que este valor tiene una
interpretaci√≥n limitada. El valor p es algo elevado (es decir, supera los niveles
utilizados normalmente como corte para evitar errores tipo 1), lo que indica que no
podr√≠amos afirmar con certeza su diferencia de cero. Por otro lado, el coeficiente en
educaci√≥n indica que cada a√±o en educaci√≥n (una unidad adicional) se traducir√≠a en un
aumento de 0.54136 unidades en salario (si son pesos, pues 54 chavos). El valor p es bajo
(o el puntaje t es elevado, distante a 2, y mucho mayor que el error est√°ndar), lo que
indica con cierta certeza que el estimado no es cero. Vemos esto acompa√±ado con
asteriscos, simbolizando el nivel de significancia, atado al valor seleccionado $\alpha$,
el punto de corte anteriormente mencionado.

El modelo luego contin√∫a reportando otros diagn√≥sticos generales:

-   `Residual standard error` (Error est√°ndar de los residuos): m√°s o menos 3.378

    -   Indica la variabilidad promedio de los residuos; en otras palabras, mide la
        precisi√≥n del modelo. Los modelos lineales incluyen un t√©rmino estoc√°stico, que
        captura las desviaciones de nuestra relaci√≥n predicha con las observaciones. En
        este caso, el valor de 3.378, sugiere que las predicciones individuales del
        salario pueden variar en promedio ¬±3.378 unidades del valor real a trav√©s del
        espacio de nuestras observaciones.

-   `Degrees of freedom` (Grados de libertad): 524 -- Calculado como el n√∫mero de
    observaciones menos el n√∫mero de par√°metros estimados (n - k). El objeto utilizado
    para an√°lisis ten√≠a 526 filas, y estimamos la pendiente y el intercepto.

-   El coeficiente de determinaci√≥n ($R^2$) nos da 0.1648, que indica que el 16.48% de la
    variabilidad se explica con el modelo: es decir una proporci√≥n de la varianza
    explicada. El *R cuadrado ajustado* es similar, pero penaliza la inclusi√≥n de t√©rminos
    adicionales, algo que el $R^2$ no har√°: al seguir a√±adiendo variables, $R^2$ seguir√°
    aumentando, sean buenas o no las adiciones; el $R^2$ *ajustado* es preferido para
    an√°lisis en regresi√≥n m√∫ltiple.

-   El estad√≠stico F y el valor p global eval√∫a la hip√≥tesis nula de que todos los
    coeficientes sean iguales a cero.

Al objeto creado para almacenar el modelo lineal le podemos hacer an√°lisis varios. Por
ejemplo, podr√≠amos hacerle el an√°lisis de varianzas. Este se calcula con la funci√≥n
`anova()`:

```{r}
anova(mod1) # para imprimir el an√°lisis de varianzas
```

Este an√°lisis descompone la variabilidad total de la variable dependiente (`wage` en este
caso) en componentes atribuibles al modelo y a los residuos. ¬øPara qu√© har√≠amos esto?
Quiz√°s en este caso simple no parezca muy revelador (aunque da m√°s detalles, por ejemplo,
sobre el c√≥mputo que lleva al puntaje F, viendo). Si tenemos variables categ√≥ricas con m√°s
de dos niveles, podremos aclarar m√°s el impacto de la variable en general, en lugar de
meramente entender la diferencia de distintos grupos en referencia a un valor base.

El objeto creado a trav√©s de la funci√≥n `lm()`, de clase lm, tiene en s√≠ doce componentes:

```{r, adentro del modelo}
names(mod1)

```

Se puede acceder a ellos como a las variables dentro de un objeto, utilizando el operador
`$` entre el objeto y el elemento. Por ejemplo, para extraer los coeficientes estimados se
escribe lo siguiente:

```{r, coeficientes}
mod1$coefficients

coef(summary(mod1))
```

Notemos que la funci√≥n de `coef`, que extrae coeficientes de objetos creados por funciones
de modelaje estad√≠stico, nos retorna una matriz con cuatro columnas y la cantidad de filas
adecuadas para los par√°metros estimados, en este caso dos.

Una variaci√≥n podr√≠a ser:

```{r}
coefficients(mod1)

summary(mod1)$coefficients #noten, es igual en resultado a la funci√≥n de coef(summary(modelo))
```

Para consultar la estimaci√≥n de un coeficiente de regresi√≥n se utilizan *corchetes [ ]* y
se indica la ubicaci√≥n del mismo dentro de la salida del `summary()`. Por ejemplo, al alfa
gorro o sombrerito (es decir, $\hat{\alpha}$, alfa estimado) se podr√≠a sacar llamando a la
*[primera fila, primera columna]*:

```{r}
ahat <- coef(summary(mod1))[1,1]
ahat
```

Y el beta gorro $\hat{\beta}$ o beta estimado se podr√≠a obtener llamando a la *[segunda
fila, primera columna]*:

```{r}
bhat <- coef(summary(mod1))[2,1]
bhat
```

Del modelo podemos sacar tambi√©n:

```{r}
coeficientes <- mod1$coefficients #vector de coeficientes estimados
ygorro <- mod1$fitted.values #valores predichos
resid <- mod1$residuals #residuos
#los valores estimados o predichos tambi√©n se pueden sacar con la funci√≥n predict():
ygor1<- predict(mod1) 
head(ygor1)
head(ygorro)
all.equal(ygor1, ygorro)
```

Agregaremos al conjunto de datos original, las predicciones, en este caso salario
estimado, con la funci√≥n `predict()`.

```{r}
datos2$predicciones <- predict(mod1) 
head(datos2, 6)
```

El gr√°fico de dispersi√≥n puede establecerse

```{r}
ggplot(datos2, aes(x = educ, y = wage)) + 
  geom_point() +
  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, col = 'dodgerblue1') +
  theme_light() +
  ggtitle("Relaci√≥n entre salario y educaci√≥n")
```

Si quisi√©ramos un gr√°fico de dispersi√≥n interactivo, podemos usar plotly. As√≠,
posicion√°ndose encima de cada observaci√≥n, se ven los valores de (x, y) para cada uno de
los individuos. Para construir dicho gr√°fico se necesita la funci√≥n ggplotly() del paquete
plotly.

```{r}
library(plotly)

ggplotly(data = datos2, x = ~ educ, y = ~ wage)
```

## Supuestos de modelos lineales

Ahora, a la hora de evaluar si nuestro modelo es bueno, adem√°s de los estimados que vimos
sobre el ajuste del modelo o su capacidad explicativa de la varianza, tenemos que revisar
las asunciones o presunciones que hace un modelo sobre los datos que eval√∫a. En la
evaluaci√≥n de un modelo lineal, tenemos cinco.

1.  **Linealidad**: La relaci√≥n entre las variables dependiente (y) e independiente(s) (x)
    debe ser lineal en los par√°metros. Esto significa que el modelo es lineal en los
    coeficientes, aunque las variables en s√≠ mismas puedan estar transformadas (por
    ejemplo, mediante logaritmos).

2.  **Independencia de los errores**: Los residuos o errores (la diferencia entre los
    valores observados y los valores predichos por el modelo) deben ser independientes
    entre s√≠. Esto significa que no debe haber correlaci√≥n entre los errores.

3.  **Homoscedasticidad**: La varianza de los errores debe ser constante a lo largo de
    todos los valores de las variables independientes. Esto implica que la dispersi√≥n de
    los residuos debe ser m√°s o menos la misma a lo largo del rango de valores de la
    variable independiente. En caso de heteroscedasticiddad, las estimaciones se tornan
    ineficientes y los errores est√°ndar incorrectos.

4.  **Normalidad de los errores**: Los errores deben seguir una distribuci√≥n normal. Este
    supuesto es importante para la realizaci√≥n de pruebas de hip√≥tesis y la construcci√≥n
    de intervalos de confianza. Cabe se√±alar que en este caso, los estimadores de los
    coeficientes siguen siendo insesgados incluso si los errores no fueran normales. La
    falta de normalidad afecta principalmente la inferencia estad√≠stica.

5.  **No multicolinealidad**: Las variables independientes no deben estar altamente
    correlacionadas entre s√≠. La multicolinealidad puede dificultar la estimaci√≥n precisa
    de los coeficientes de regresi√≥n.

De violarse estas presunciones, el modelo estar√° sesgado y sus resultados no ser√°n del
todo fiables.

Podremos revisar algunos de estos a trav√©s de varios m√©todos: Podemos por ejemplo calcular
los residuos del modelo simple y los agregamos al conjunto de datos (*datos2*) de la
siguiente forma:

```{r}
datos2$residuos <- datos2$wage - datos2$predicciones

head(datos2, 5)
```

Esto es lo mismo que R hace internamente, y se puede llamar con la funci√≥n `residuals`:

```{r}
datos2$residuosmod <- residuals(mod1)
head(datos2, 5)
```

Comparamos los residuos calculados manualmente con los que nos dio el modelo. Otra forma
de extraerlos es llamando con acceso `$` en el objeto a los residuos. `mod1$residuals` es
equivalente a `residuals(mod1)`. Sin embargo, utilizar la funci√≥n `residuals()` es
generalmente preferible porque es m√°s compatible con diferentes tipos de modelos y objetos
en R.

A√±adimos al gr√°fico el elemento de los valores estimados de y, $\hat{y}_i$, en rojo y
muestro los residuos $\hat{\varepsilon}_i$:

```{r}
ggplot(datos2, aes(x = educ, y = wage)) +
  geom_point() +
  geom_segment(aes(xend = educ, yend = predicciones), color = 'red', linetype = 'dashed') +
  geom_point(aes(y = predicciones), color = 'red') +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  theme_light() +
  labs(title = "Salario vs Educaci√≥n con Residuos", x = "Educaci√≥n", y = "Salario")
```

Podemos realizar un gr√°fico de dispersi√≥n para inspeccionar de forma gr√°fica los residuos.

```{r}
plot(datos2$residuos)
```

El comando `plot(datos2$residuos)` simplemente grafica los residuos en funci√≥n de su
√≠ndice, lo cual puede no ser muy informativo (aunque ciertamente se aprecia alg√∫n patr√≥n
aglomer√°ndose mucho por debajo de cero, y dispers√°ndose hacia valores m√°s altos). Ser√≠a
m√°s √∫til graficar los residuos contra los valores predichos o contra la variable
independiente para detectar patrones que indiquen violaciones de los supuestos del modelo.

```{r}
ggplot(datos2, aes(x = predicciones, y = residuos)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(title = "Residuos vs Valores Predichos", x = "Valores Predichos", y = "Residuos")
```

Hasta ahora vemos problemas en la dispersi√≥n de los residuos, pues no parecen estar
dispersos como ruido alrededor de cero. Podr√≠amos mejorar esto algo al ver la asimetr√≠a
(skewness) de la distribuci√≥n

```{r}
library(e1071)  # para la funci√≥n skewness
par(mfrow = c(1, 2))  # divide el √°rea de gr√°ficos en 2 columnas

plot(density(datos2$wage), main = "Gr√°fico de densidad: salario", ylab = "Frecuencia", sub = paste("Asimetr√≠a:", round(e1071::skewness(datos2$wage), 2)))  # density para 'salario'

polygon(density(datos2$wage), col = "red")

plot(density(datos2$residuos), main = "Gr√°fico de densidad: residuos", ylab = "Frecuencia", sub = paste("Asimetr√≠a:", round(e1071::skewness(datos2$residuos), 2)))  # density para los 'residuos'

polygon(density(datos2$residuos), col = "red")
```

Volveremos m√°s tarde con la resoluci√≥n de este problema, pero por ahora continuaremos con
este modelo tal cual para mostrar otros elementos √∫tiles.

## Predicciones con estimados

Generaremos la predicci√≥n puntual y el intervalo de confianza para una educaci√≥n promedio
en a√±os.

```{r}
mean(datos2$educ)
```

Primero, calculamos el salario esperado para una persona con la educaci√≥n promedio que es
12.56 a√±os: $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}$.

```{r}
sal_pred <- ahat + bhat * 12.56274
sal_pred
```

La funci√≥n predict() se puede igual usar para obtener la predicci√≥n puntual:

```{r}
nuevo <- data.frame(educ = mean(datos2$educ))
sal_pred <- predict(mod1, newdata = nuevo)
sal_pred
```

Ahora, calculamos la predicci√≥n y los intervalos para una persona con 15 a√±os de educaci√≥n
para ilustrar c√≥mo var√≠an estos valores con diferentes niveles de educaci√≥n. Entonces
($ùë•=15$). El argumento `interval = prediction` devuelve el valor para la predicci√≥n
puntual junto a su intervalo de predicci√≥n, que estima el rango en el cual caer√° una nueva
observaci√≥n individual con un cierto nivel de confianza.

```{r}
nuevo <- data.frame(educ = 15)
future_y <- predict(object = mod1, newdata = nuevo, interval = "prediction", level = 0.95)
future_y
```

Si desean obtener solo la predicci√≥n puntual, pueden omitir el argumento `interval`.

Luego, generamos el intervalo de confianza para $ùê∏(ùë¶\midùë•)$. en este caso, debemos cambiar
el argumento a `interval = "confidence"`. El intervalo de confianza es para la media
esperada de la variable dependiente dado un valor espec√≠fico de la independiente, aqu√≠ 15
a√±os de educaci√≥n.

```{r}
future_esp_y <- predict(object = mod1, newdata = nuevo, interval = "confidence", level = 0.95)
future_esp_y <- as.data.frame(future_esp_y)

IC_inf_esp_y <- future_esp_y$lwr
IC_sup_esp_y <- future_esp_y$upr
```

Agregando los pedazos

```{r}
# Calcular intervalos de predicci√≥n para todas las observaciones
future_y_all <- predict(object = mod1, newdata = datos2, interval = "prediction", level = 0.95)
future_y_all <- as.data.frame(future_y_all)

# Calcular intervalos de confianza para todas las observaciones
future_esp_y_all <- predict(object = mod1, newdata = datos2, interval = "confidence", level = 0.95)
future_esp_y_all <- as.data.frame(future_esp_y_all)

# Combinar todos los datos en un solo data frame
nuevos_datos <- cbind(datos2, future_y_all, 
                      IC_inf_esp_y = future_esp_y_all$lwr, 
                      IC_sup_esp_y = future_esp_y_all$upr)
```

Finalmente, generamos los gr√°ficos correspondientes con los intervalos de confianza para
la predicci√≥n puntual (`IC_y`) y para el valor esperado (`IC_esp_y`) con el siguiente
c√≥digo:

```{r}
IC_y <- ggplot(nuevos_datos, aes(x = educ, y = wage)) +
  geom_point() +
  geom_line(aes(y = lwr), color = "red", linetype = "dashed") +
  geom_line(aes(y = upr), color = "red", linetype = "dashed") +
  geom_smooth(method = lm, formula = y ~ x, se = TRUE, level = 0.95, col = 'blue', fill = 'pink2') +
  theme_light() + 
  ggtitle("Predicci√≥n de y al 95%")

IC_esp_y <- ggplot(nuevos_datos, aes(x = educ, y = wage)) +
  geom_point() +
  geom_line(aes(y = IC_inf_esp_y), color = "blue", linetype = "dashed") +
  geom_line(aes(y = IC_sup_esp_y), color = "blue", linetype = "dashed") +
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = 'blue') +
  theme_light() + 
  ggtitle("Intervalo de Confianza de E(y|x) al 95%")
```

Imprimimos los gr√°ficos uno al lado del otro, para poder compararlos mejor. ¬øCu√°l de los
dos tiene mayor amplitud?

```{r}
library(gridExtra)

grid.arrange(IC_esp_y, IC_y, ncol = 2, nrow = 1)
```

Al comparar ambos gr√°ficos, observamos que el intervalo de predicci√≥n es m√°s amplio que el
intervalo de confianza. Esto es esperado, ya que el intervalo de predicci√≥n considera la
variabilidad adicional de las observaciones individuales, mientras que el intervalo de
confianza se centra √∫nicamente en la precisi√≥n de la estimaci√≥n del valor medio esperado.

Veamos, a trav√©s de la comparaci√≥n de dos gr√°ficos el impacto que tiene el nivel de
confianza en la amplitud de los intervalos. Para ello, tendremos que descargar e instalar
algunas librer√≠as. Se presenta primero el c√≥digo y luego los gr√°ficos obtenidos.

```{r}
# Instalar y cargar las librer√≠as necesarias si no las tiene
#install.packages("jtools")
#install.packages("gridExtra")
library(jtools)
library(gridExtra)

# Crear los gr√°ficos con diferentes niveles de confianza
a <- plot_summs(mod1, scale = TRUE, plot.distributions = FALSE, ci_level = 0.99) +
  ggtitle("Intervalos de confianza al 99%")

b <- plot_summs(mod1, scale = TRUE, plot.distributions = FALSE, ci_level = 0.95) +
  ggtitle("Intervalos de confianza al 95%")

c <- plot_summs(mod1, scale = TRUE, plot.distributions = FALSE, ci_level = 0.90) +
  ggtitle("Intervalos de confianza al 90%")

d <- plot_summs(mod1, scale = TRUE, plot.distributions = FALSE, ci_level = 0.70) +
  ggtitle("Intervalos de confianza al 70%")

# Mostrar los gr√°ficos uno debajo del otro
grid.arrange(a, b, c,d, ncol = 2, nrow = 2)
```

Al comparar los gr√°ficos, observamos que un nivel de confianza m√°s alto (99%) produce
intervalos m√°s amplios, ya que estamos buscando abarcar una mayor proporci√≥n de posibles
valores verdaderos de los coeficientes. Por el contrario, ir bajando hacia un nivel de
confianza menor (70%) resulta en intervalos m√°s estrechos.

# Regresi√≥n m√∫ltiple

En esta secci√≥n, queremos evaluar c√≥mo varias variables independientes se relacionan con
una variable dependiente. Espec√≠ficamente, analizaremos c√≥mo la educaci√≥n y la antig√ºedad
influyen en el salario. Luego a√±adiremos otras variables adicionales, como se√±al√°ramos m√°s
temprano:

-   wage: salario promedio por hora.

-   educ: a√±os de educaci√≥n.

-   exper: a√±os de experiencia potencial.

-   tenure: a√±os con el empleador actual (antig√ºedad).

-   nonwhite: es igual a 1 si la persona no es blanca, 0 si no.

-   female: es igual a 1 si la persona es mujer, 0 si no

-   married: es igual a 1 si la persona es casada, 0 si no.

Digamos que queremos evaluar la relaci√≥n de varias variables con la dependiente. Podemos
calcular las correlaciones de los pares de variables, indicando que queremos trabajar con
3 decimales:

```{r}
round(cor(base1, method = "pearson"), 3)

```

Antes de estimar el modelo de regresi√≥n m√∫ltiple, es √∫til explorar las relaciones entre
las variables mediante la matriz de correlaciones. Esto nos ayuda a detectar posibles
problemas de multicolinealidad y a entender las relaciones bivariadas. En este caso,
haremos una restricci√≥n a tres variables, ayudando a mejorar la legibilidad.

```{r}
# Seleccionamos las variables de inter√©s
variables_interes <- base1[, c("wage", "educ", "antig√ºedad")]

# Calculamos la matriz de correlaciones y redondeamos a 3 decimales
matriz_correlaciones <- round(cor(variables_interes, method = "pearson"), 3)
matriz_correlaciones
```

Interpretaci√≥n:

-   Salario y Educaci√≥n: Un coeficiente positivo indica que a mayor nivel educativo, el
    salario tiende a ser mayor. La fuerza de esta relaci√≥n parece ser moderada, 0.406.
-   Salario y Antig√ºedad: Un coeficiente positivo sugiere que con m√°s a√±os de antig√ºedad,
    el salario tambi√©n aumenta. Parece ser de una fuerza similar a educaci√≥n.
-   Educaci√≥n y Antig√ºedad: El coeficiente es negativo, y peque√±o. Entre estas variables
    la relaci√≥n no es clara en fuerza pero parecen ir en direcciones opuestas. Es menos
    probable que haya multicolinealidad entre estas variables independientes.

Para visualizar la relaci√≥n conjunta entre las tres variables, utilizamos gr√°ficos
tridimensionales.

```{r}
library(ggplot2)
library(plotly)
attach(base1)
plot_ly(x = educ, y = antig√ºedad, z = wage, type = "scatter3d", color = wage) |> 
  layout(scene = list(xaxis = list(title = 'educaci√≥n (en a√±os)'),
                      yaxis = list(title = 'antig√ºedad (en a√±os)'),
                      zaxis = list(title = 'Salario (en USD/h)')))
```

Este gr√°fico interactivo permite rotar y explorar la relaci√≥n entre las variables desde
diferentes √°ngulos, facilitando la identificaci√≥n de patrones. El pr√≥ximo es menos
interactivo, pero logra un efecto similar:

```{r}
library(scatterplot3d)
graf <- scatterplot3d(x = educ, y = antig√ºedad, z = wage, pch = 16, 
              cex.lab = 1, highlight.3d = TRUE, type = "h", 
              xlab = 'A√±os de educaci√≥n',
              ylab = 'Antig√ºedad (a√±os)', 
              zlab = 'Salario (USD/h)')
```

En este caso, las l√≠neas verticales ayudan a visualizar la posici√≥n de cada punto en el
espacio tridimensional.

Ahora, estimamos un modelo de regresi√≥n lineal m√∫ltiple para cuantificar el efecto de la
educaci√≥n y la antig√ºedad en el salario.

```{r}
# Estimamos el modelo de regresi√≥n m√∫ltiple
mod2 <- lm(wage ~ educ + antig√ºedad, data = base1)

# Resumen del modelo
summary(mod2)
```

La salida es similar al modelo anterior, con filas adicionales para cada coeficiente
estimado adicional. El intercepto o constante representa el salario promedio cuando la
educaci√≥n y la antig√ºedad son cero (interpretaci√≥n limitada en este contexto). Educ indica
el cambio esperado en el salario por cada a√±o adicional de educaci√≥n, manteniendo
constante la antig√ºedad en su valor esperado. Antig√ºedad muestra el cambio esperado en el
salario por cada a√±o adicional de antig√ºedad, manteniendo constante la educaci√≥n. En este
caso vemos que un a√±o adicional de educaci√≥n se traduce a unos 57 chavos adicionales en
ingreso, mientras que antig√ºedad a√±ade cerca de 19 chavos al ingreso.

Todos los coeficientes reportan valores t relativamente grandes en relaci√≥n a sus errores
est√°ndar, y rechazamos en cada caso la hip√≥tesis nula de no ser significativamente
distinto a un efecto nulo. Notamos que tanto $R^2$ y $R^2$ *ajustado* han aumentado ambos,
aunque la distancia entre ambos ahora es algo m√°s notable: duplicamos la varianza
explicada.

¬øC√≥mo se ve esto? A√±adimos el plano de regresi√≥n al gr√°fico 3D para visualizar c√≥mo el
modelo ajusta los datos.

```{r}
graf <-  scatterplot3d(x = educ, y = antig√ºedad, z = wage, pch = 16, 
                     cex.lab = 1, highlight.3d = TRUE, type = "h", 
                     xlab = 'A√±os de educaci√≥n',
                     ylab = 'Experiencia (a√±os)', 
                     zlab = 'Salario (USD/h)')

graf$plane3d(mod2, lty.box = "solid", col = 'mediumblue')
```

El plano representa las predicciones del modelo para diferentes combinaciones de educaci√≥n
y antig√ºedad. Podemos observar qu√© tan bien el plano ajusta a los datos reales, intentando
cortar por un espacio vectorial estimado linealmente.

El ANOVA nos permite evaluar la significancia global del modelo y la contribuci√≥n de cada
variable independiente.

```{r}
anova(mod2)
```

-   Sumas de Cuadrados (Sum Sq): Indican la variabilidad explicada por cada variable
    independiente y la variabilidad residual.

Para garantizar la validez de nuestro modelo de regresi√≥n m√∫ltiple, es fundamental
verificar que se cumplen los supuestos b√°sicos del modelo lineal. A continuaci√≥n,
revisaremos cada uno de estos supuestos en el orden mencionado anteriormente, aplicando
pruebas diagn√≥sticas y proporcionando interpretaciones detalladas.

## An√°lisis de suposiciones

Antes evaluamos elementos de los residuos, pero lo hicimos por encima. R tiene en forma
base unos gr√°ficos que ayudan a informarnos sobre si los modelos tienen problemas en c√≥mo
est√°n siendo aplicados.

Primero, podemos evaluar el modelo gr√°ficamente con la funci√≥n `plot()`:

```{r}
plot(mod2)
```

1.  Residuos vs Ajustados (Residuals vs Fitted):

-   Qu√© muestra: Este gr√°fico representa los residuos estandarizados en funci√≥n de los
    valores ajustados por el modelo.

-   Interpretaci√≥n: Sirve para detectar patrones no lineales y evaluar la homogeneidad de
    la varianza (homocedasticidad). Si los puntos se distribuyen aleatoriamente alrededor
    de la l√≠nea horizontal (residuo = 0) sin formar patrones, indica que el modelo es
    adecuado. Patrones sistem√°ticos o formas espec√≠ficas (como una curva) sugieren que el
    modelo no captura adecuadamente la relaci√≥n entre las variables, o que existe
    heterocedasticidad.

2.  Gr√°fico Q-Q Normal (Normal Q-Q Plot):

-   Qu√© muestra: Compara la distribuci√≥n de los residuos estandarizados con una
    distribuci√≥n normal te√≥rica.
-   Interpretaci√≥n: Eval√∫a la normalidad de los residuos, un supuesto clave en modelos
    lineales. Si los puntos siguen aproximadamente una l√≠nea recta, los residuos se
    distribuyen normalmente. Desviaciones significativas de la l√≠nea recta indican que los
    residuos no son normales, lo que puede afectar la validez de los intervalos de
    confianza y pruebas de hip√≥tesis.

3.  Escala-Ubicaci√≥n (Scale-Location Plot):

-   Qu√© muestra: Grafica la ra√≠z cuadrada de los residuos estandarizados
    `(sqrt(\|Residuos estandarizados\|))` frente a los valores ajustados.
-   Interpretaci√≥n: Ayuda a verificar la homocedasticidad. Una dispersi√≥n uniforme de
    puntos sugiere varianza constante de los residuos. Si los puntos muestran un patr√≥n
    (por ejemplo, se ensanchan o estrechan a lo largo del eje de los ajustados), indica
    heterocedasticidad, lo que puede afectar la eficiencia de los estimadores.

4.  Residuos Estandarizados vs Apalancamiento (Residuals vs Leverage):

-   Qu√© muestra: Muestra los residuos estandarizados frente al apalancamiento de cada
    observaci√≥n, con curvas de distancia de Cook superpuestas.
-   Interpretaci√≥n: Identifica observaciones influyentes que tienen un gran impacto en el
    ajuste del modelo. Puntos con alto apalancamiento y residuos grandes pueden
    distorsionar los resultados. Las l√≠neas de distancia de Cook ayudan a detectar estos
    puntos. Observaciones m√°s all√° de estas l√≠neas merecen una revisi√≥n adicional.

Notamos que el modelo tiene sus problemas, violentando varios de los principios se√±alados
antes.

### Linealidad

*La relaci√≥n entre las variables independientes y la variable dependiente es lineal*.

```{r}
# Gr√°ficos de dispersi√≥n con l√≠nea de regresi√≥n para cada variable independiente
library(ggplot2)

ggplot(base1, aes(x = educ, y = wage)) +
  geom_point() +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Salario vs Educaci√≥n", x = "A√±os de Educaci√≥n", y = "Salario") +
  theme_minimal()

ggplot(base1, aes(x = antig√ºedad, y = wage)) +
  geom_point() +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Salario vs Antig√ºedad", x = "A√±os de Antig√ºedad", y = "Salario") +
  theme_minimal()
```

### Independencia de errores

Realizando la prueba de Durbin-Watson para verificar la autocorrelaci√≥n estoc√°stica. La
hip√≥tesis nula es que los residuos no tienen autocorrelaci√≥n (es posible una alternativa,
editando opciones), es decir que los residuos son normales.

```{r}
# Prueba de Durbin-Watson
library(lmtest)

dwtest(mod2)
```

En este caso la prueba rechaza la nula. Tenemos problemas en nuestros residuos tal cual
modelados al presente, pues se viola el supuesto de independencia.

### Homoscedasticidad

```{r}
# Gr√°fico de Residuos vs Valores Ajustados
ggplot(data = base1, aes(x = mod2$fitted.values, y = mod2$residuals)) +
  geom_point() +
  geom_smooth(method = "loess", col = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Valores Ajustados", x = "Valores Ajustados", y = "Residuos") +
  theme_minimal()

# Prueba de Breusch-Pagan
library(lmtest)
bptest(mod2)
```

Vemos una representaci√≥n visual (con `ggplot()`) de los residuos contrastados a los
valores ajustados. La prueba Breusch Pagan tiene la hip√≥tesis nula siguiente: varianza
constante. Por consiguiente, rechazarla es encontrar heteroscedasticidad.

### Normalidad de errores

El supuesto a verificar ahora (aunque no est√° yendo bien en general) es saber si los
errores se distribuyen normalmente. Hay varias opciones gr√°ficas y con pruebas
estad√≠sticas: el gr√°fico Q-Q Plot, el histograma de los residuos, el diagrama de densidad
de residuos, y la prueba Shapiro-Wilk.

```{r}
# Gr√°fico Q-Q de los residuos
qqnorm(mod2$residuals)
qqline(mod2$residuals, col = "red")

# Histograma de los residuos
hist(mod2$residuals, breaks = 20, main = "Histograma de Residuos", xlab = "Residuos", col = "lightblue")

# Gr√°fico de densidad de los residuos con asimetr√≠a
library(e1071)  # Para la funci√≥n skewness()

plot(density(mod2$residuals), 
     main = "Gr√°fico de Densidad de Residuos", xlab = "Residuos", 
     sub = paste("Asimetr√≠a:", round(skewness(mod2$residuals), 2)))  
polygon(density(mod2$residuals), col = "red")
```

Por ejemplo, estos residuos distan de ser normales. Las cuantilas de los residuos distan
de la normalidad esperada, por *MUCHO*. Verificamos

```{r, Prueba Shapiro}
shapiro.test(mod2$residuals)
```

La hip√≥tesis nula de la prueba de Shapiro-Wilk es que estamos ante normalidad en los
errores. Esto confirma lo sugerido por el an√°lisis gr√°fico y estad√≠stico precedente.

En los siguientes gr√°ficos se muestran los residuos contra cada uno de los regresores, los
cuales se realizan con el siguiente c√≥digo:

```{r}
plot1 <- ggplot(data = base1, aes(educ, mod1$residuals)) +
  geom_point() + 
  geom_smooth(color = "firebrick") + 
  geom_hline(yintercept = 0) +
  theme_bw()

plot2 <- ggplot(data = base1, aes(antig√ºedad, mod1$residuals)) +
  geom_point() + 
  geom_smooth(color = "firebrick") + 
  geom_hline(yintercept = 0) +
  theme_bw()

grid.arrange(plot1, plot2)
```

El an√°lisis gr√°fico indica problemas de heteroscedasticidad y de correlaci√≥n entre los
residuos y el nivel de los regresores.

### Multicolinealidad

```{r}
# Matriz de correlaciones entre variables independientes
vars_indep <- base1[, c("educ", "antig√ºedad")]
cor(vars_indep)

# C√°lculo del VIF
library(car)
vif(mod2)
```

Los valores no son particularmente altos en correlaci√≥n (alto es cerca de 1 √≥ -1, bajo es
cerca de 0). En nuestro caso, la correlaci√≥n entre educaci√≥n y antig√ºedad es baja
(-0.056), lo que sugiere baja multicolinealidad. La prueba del factor de inflaci√≥n de
varianza (VIF) se puede usar para verificar num√©ricamente si hay multicolinealidad.

### Identificar observaciones influyentes

Aunque no es un supuesto b√°sico, es importante identificar puntos que puedan influir
excesivamente en el modelo. Esto se puede verificar: - Analizando el gr√°fico de Residuos
Estandarizados vs Apalancamiento. - Calculando la Distancia de Cook.

```{r}
# Gr√°fico de Residuos Estandarizados vs Apalancamiento
plot(mod2, which = 5)

# Gr√°fico de Distancia de Cook
plot(mod2, which = 4)

# Identificar observaciones con alta Distancia de Cook
cooksd <- cooks.distance(mod2)
influential <- as.numeric(names(cooksd)[(cooksd > (4 / nrow(base1)))])
influential

# Mostrar las observaciones influyentes
base1[influential, ]
```

Puntos fuera de las l√≠neas de referencia pueden ser influyentes. Los gr√°ficos se√±alan los
valores que parecen distantes al resto de lo observado. En estos casos, se recomienda
revisar las observaciones para verificar si hay errores en los datos o si representan
casos especiales.

### An√°lisis de residuos contra variables independientes

Es √∫til graficar los residuos contra cada una de las variables independientes para
detectar patrones espec√≠ficos.

```{r}
# Residuos vs Educaci√≥n
plot1 <- ggplot(data = base1, aes(x = educ, y = mod2$residuals)) +
  geom_point() + 
  geom_smooth(method = "loess", color = "firebrick", se = FALSE) + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Educaci√≥n", x = "Educaci√≥n", y = "Residuos") +
  theme_bw()

# Residuos vs Antig√ºedad
plot2 <- ggplot(data = base1, aes(x = antig√ºedad, y = mod2$residuals)) +
  geom_point() + 
  geom_smooth(method = "loess", color = "firebrick", se = FALSE) + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Antig√ºedad", x = "Antig√ºedad", y = "Residuos") +
  theme_bw()

# Mostrar los gr√°ficos lado a lado
library(gridExtra)
grid.arrange(plot1, plot2, ncol = 2)
```

-   Residuos vs Educaci√≥n: Si observamos un patr√≥n sistem√°tico, podr√≠a indicar que la
    relaci√≥n no es completamente lineal o que hay variables omitidas.
-   Residuos vs Antig√ºedad: Patrones similares pueden sugerir problemas de
    heterocedasticidad o relaciones no lineales.

### ¬øQu√© hacemos?

Como detectamos problemas de heterocedasticidad y normalidad de errores, podemos probar
transformando la variable dependiente. Por ahora a√±adir√© unas variables adicionales y
tambi√©n haremos unas variaciones con transformaciones (una de estas ya estaba en el
conjunto de datos). Es posible que al incorporar variables relevantes que puedan explicar
mejor la variabilidad en el salario, mejore el ajuste del modelo. Por otro lado, es
posible que transformar la variable dependiente o algunas independientes para corregir
violaciones a los supuestos del modelo lineal arreglen estos problemas.

A continuaci√≥n, implementaremos estas estrategias paso a paso.

A√±adiremos variables que podr√≠an influir significativamente en el salario, como:

-   married: Estado civil (1 si est√° casado, 0 si no).
-   female: G√©nero (1 si es mujer, 0 si es hombre).
-   nonwhite: Etnicidad (1 si no es blanco, 0 si es blanco).

```{r}
# Actualizamos el modelo a√±adiendo la variable 'married'
mod3 <- update(mod2, wage ~ educ + antig√ºedad + married)

# Resumen del modelo
summary(mod3)
```

-   Intercepto (-2.52575): Representa el salario promedio cuando educ, antig√ºedad y
    married son cero. Aunque no es interpretable en este contexto, es necesario para el
    modelo.
-   Educaci√≥n (0.55614): Por cada a√±o adicional de educaci√≥n, el salario promedio aumenta
    en aproximadamente 56 chavos, manteniendo constantes la antig√ºedad y el estado civil.
-   Antig√ºedad (0.17482): Por cada a√±o adicional de antig√ºedad con el empleador actual, el
    salario promedio aumenta en aproximadamente \$0.17, manteniendo constantes las otras
    variables.
-   Casado (0.89235): Estar casado se asocia con un aumento promedio de 89 chavos en el
    salario, manteniendo constantes la educaci√≥n y la antig√ºedad.
-   Significancia estad√≠stica: Todos los coeficientes son estad√≠sticamente significativos
    (p \< 0.05), lo que indica que tienen un efecto significativo en el salario.
-   Ajuste del modelo:
    -   R-cuadrado (0.3149): El modelo explica aproximadamente el 31% de la variabilidad
        en el salario, lo que es una mejora respecto a los modelos anteriores.
    -   Error est√°ndar residual (3.066): Es menor que en los modelos previos, lo que
        indica un mejor ajuste.

Para visualizar los coeficientes del modelo, podemos utilizar la librer√≠a `modelsummary` y
la funci√≥n `modelplot()`:

```{r}
library(modelsummary)
modelplot(mod3, coef_omit = "Intercept", color = "blue", size = 1) +
  labs(title = "Coeficientes del Modelo 3")
```

*Nota*: Al incluir variables categ√≥ricas como married, es importante considerar que sus
coeficientes representan diferencias respecto a la categor√≠a de referencia.

#### Estandarizaci√≥n de Variables

Para comparar los efectos de las variables en la misma escala, podemos estandarizar las
variables num√©ricas:

```{r}
base1_estandarizado <- base1 |>
  mutate(across(where(is.numeric), scale))

# Ajustamos el modelo con los datos estandarizados
mod3_est <- lm(wage ~ educ + antig√ºedad + married, data = base1_estandarizado)

# Visualicemos los coeficientes estandarizados
modelplot(mod3_est, coef_omit = "Intercept", color = "blue", size = 1) +
  labs(title = "Coeficientes Estandarizados del Modelo 3")
```

Los coeficientes estandarizados permiten comparar directamente el efecto relativo de cada
variable en el salario. Un coeficiente m√°s grande en valor absoluto indica un efecto
mayor.

Al ejecutar `plot(mod3)`, obtenemos los gr√°ficos diagn√≥sticos para evaluar los supuestos
del modelo.

```{r}
plot(mod3)
```

```{r}
#library(lmtest)
bptest(mod3)
shapiro.test(mod3$residuals)
```

Los residuos todav√≠a siguen distando de ser una distribuci√≥n normal. Ampliaremos el modelo
con otras variables que se√±al√°ramos antes como de inter√©s.

```{r}
# Creamos un nuevo modelo incluyendo 'female' y 'nonwhite'
mod4 <- lm(wage ~ educ + antig√ºedad + married + female + nonwhite, data = base1)

# Resumen del modelo
summary(mod4)
```

-   Educaci√≥n y antig√ºedad: Siguen siendo significativas y positivas.
-   Married (0.68258): Estar casado se asocia con un aumento promedio de \$0.68 en el
    salario, manteniendo constantes las dem√°s variables.
-   Female (-1.71149): Ser mujer se asocia con una disminuci√≥n promedio de \$1.71 en el
    salario, manteniendo constantes las dem√°s variables. Este efecto es estad√≠sticamente
    significativo.
-   Nonwhite (-0.06516): No es estad√≠sticamente significativo (p = 0.8788), lo que sugiere
    que, en este modelo, la variable nonwhite no tiene un efecto significativo en el
    salario.
-   Ajuste del modelo:
    -   R-cuadrado (0.3653), ajustado (0.3592): El modelo explica aproximadamente el 36%
        de la variabilidad en el salario, mejorando levemente respecto al modelo anterior.

#### An√°lisis

Es importante verificar la multicolinealidad al a√±adir nuevas variables.

```{r}
#library(car)
vif(mod4)
```

No parece haber problema de multicolinealidad, todas est√°n cerca de 1.

Verificaremos r√°pido igual los gr√°ficos del modelo.

```{r}
plot(mod4)
```

```{r}
#library(lmtest)
bptest(mod4)
shapiro.test(mod4$residuals)
```

Seguimos rechazando la hip√≥tesis nula de Shapiro-Wilk a√∫n en este caso. El problema no
parece ser resuelto por a√±adidura de otros t√©rminos. Aplicamos una transformaci√≥n
logar√≠tmica a la variable dependiente, salario.

```{r}
# Estimamos el modelo con el logaritmo del salario
mod5 <- lm(log(wage) ~ educ + antig√ºedad + married + female + nonwhite, data = base1)

# Resumen del modelo
summary(mod5)
```

Interpretaci√≥n:

Coeficientes interpretados en t√©rminos logar√≠tmicos: - Educaci√≥n (0.079483): Un a√±o
adicional de educaci√≥n se asocia con un aumento promedio del 7.95% en el salario,
manteniendo constantes las dem√°s variables. - Antig√ºedad (0.019443): Un a√±o adicional de
antig√ºedad se asocia con un aumento promedio del 1.94% en el salario. - Married
(0.146703): Estar casado se asocia con un aumento promedio del 14.67% en el salario. -
Female (-0.280401): Ser mujer se asocia con una disminuci√≥n promedio del 28.04% en el
salario. - Nonwhite (-0.002419): No es significativo. - Mejora en los supuestos del
modelo: - La transformaci√≥n logar√≠tmica puede ayudar a corregir problemas de
heterocedasticidad y normalidad de los residuos.

```{r}
plot(mod5)
```

Estos gr√°ficos se ven **distintos** a los anteriores. Los residuos parecen dispersarse de
manera m√°s uniforme alrededor de cero. Hagamos varias pruebas para verificar:

```{r}
#library(lmtest)
bptest(mod5)
shapiro.test(mod5$residuals)
```

Sigue rechaz√°ndose la nula, aunque por menos margen que antes en el caso de Breusch-Pagan
(y menos mejorado en Shapiro-Wilk).

Haremos unos √∫ltimos intentos de ajuste, como el uso de errores est√°ndar robustos (ajustan
las estimaciones de la varianza para corregir la heterocedasticidad sin cambiar los
coeficientes estimados).

```{r}
# Instalar y cargar los paquetes necesarios
#install.packages("lmtest")
#install.packages("sandwich")
library(lmtest)
library(sandwich)

# Recalcular los errores est√°ndar usando la matriz de varianza-covarianza robusta
coef(summary(mod5))
coeftest(mod5, vcov = vcovHC(mod5, type = "HC1"))
```

Los coeficientes estimados permanecen iguales en este caso, pero los errores est√°ndar y
los valores p pueden cambiar. Para m√°s informaci√≥n sobre este m√©todo recomiendo [este
art√≠culo](https://ideas.repec.org/p/ete/ceswps/ces0316.html).

Si quisi√©ramos hacer otras transformaciones adicionales, no satisfechos con estos errores
robustos, podr√≠amos buscar una transformaci√≥n Box-Cox, un tipo de transformaci√≥n de
potencia que redistribuye la variable con un logaritmo y luego eleva a un exponente √≥ptimo
para normalizar la variable. Para m√°s informaci√≥n recomiendo [este
enlace](https://datasciencetut.com/box-cox-transformation-in-r/).

```{r}
# Instalar y cargar el paquete MASS
#install.packages("MASS")
library(MASS)

# Encontrar el lambda √≥ptimo para la transformaci√≥n Box-Cox
boxcox_mod <- boxcox(mod4, plotit = TRUE)
lambda_optimo <- boxcox_mod$x[which.max(boxcox_mod$y)]
lambda_optimo

# Aplicar la transformaci√≥n Box-Cox al salario
base1$wage_boxcox <- (base1$wage^lambda_optimo - 1) / lambda_optimo

# Reestimar el modelo con la variable transformada
mod_boxcox <- lm(wage_boxcox ~ educ + antig√ºedad + married + female + nonwhite, data = base1)
summary(mod_boxcox)
# Verificar los supuestos nuevamente
shapiro.test(mod_boxcox$residuals)
bptest(mod_boxcox)
```

No rechazamos la hip√≥tesis nula de homocedasticidad por Breusch-Pagan. La
heterocedasticidad se ha mitigado en este modelo. Sin embargo, rechazamos la hip√≥tesis
nula de normalidad. Los residuos a√∫n no siguen una distribuci√≥n normal.

Tambi√©n podr√≠amos incluir otros t√©rminos polinomiales o interacciones.

```{r}
# Modelo con t√©rminos cuadr√°ticos
mod_poly <- lm(log(wage) ~ educ + I(educ^2) + antig√ºedad + I(antig√ºedad^2) + married + female + nonwhite, data = base1)

# Resumen del modelo
summary(mod_poly)

# Verificar los supuestos
shapiro.test(mod_poly$residuals)
bptest(mod_poly)
```

Tampoco rechazamos en esta transformaci√≥n la hip√≥tesis nula de homoscedasticidad. La
heteroscedasticidad se ha reducido. Pero s√≠ volvemos a notar que los errores no son
normales. Soluciones como bootstrapping podr√≠an ser √∫tiles, as√≠ como modelos
generalizados. Por ahora, podemos enfocarnos en sacar las tablas de los modelos, para uso
en \LaTeX o para otros programas.

```{r}
library(stargazer)
stargazer(mod1,mod2,mod3,mod4,mod5,mod_boxcox) #de base, para LaTeX
```

```{r}
stargazer(mod1,mod2,mod3, type="text", title = "Resultados de regresi√≥n")
stargazer(mod4,mod5,mod_boxcox, type="text", title = "Resultados de regresi√≥n 2")#de base, para LaTeX
```

# Modelos lineales generalizados

Al trabajar con datos como los de salarios en el conjunto de datos de Wooldridge, podemos
encontrar problemas con las suposiciones fundamentales de los M√≠nimos Cuadrados Ordinarios
(OLS). Aunque los modelos lineales cl√°sicos nos proporcionan resultados, estos pueden ser
problem√°ticos si los errores no est√°n bien distribuidos, es decir, si violan los supuestos
de normalidad y homocedasticidad. Esto significa que las inferencias obtenidas pueden no
ser adecuadas o estar sesgadas.

Para abordar estas limitaciones, los Modelos Lineales Generalizados (GLM) extienden el
marco de los modelos lineales al permitir una mayor flexibilidad en la relaci√≥n entre las
variables independientes y la variable dependiente. Un GLM se compone de tres elementos
clave:

1.  Predictor lineal ($\eta$): $\eta=ùëãùõΩ$

Donde ***X*** es la matriz de variables independientes y **ùõΩ** es el vector de
coeficientes.

2.  Funci√≥n de Enlace ($ùëî$): Esta es mon√≥tona y diferenciable en todo su dominio, y que
    transforma el predictor lineal $g(\mu) = \eta$. Su inversa permite obtener las
    predicciones de la variable dependiente: $ùë¶ÃÇ = ùëî‚Åª¬π(ùëãùõΩÃÇ)$, $\hat{y} = g^{-1}(\eta)$).

3.  Distribuci√≥n de Respuesta: La variable dependiente se asume que sigue una distribuci√≥n
    de la familia exponencial, denotada por $f(y | \mu)$ . Algunas distribuciones comunes
    incluyen la normal, binomial, Poisson y gamma.

Estos componentes proporcionan la flexibilidad necesaria para modelar diferentes tipos de
variables dependientes, permitiendo que el modelo capture mejor las caracter√≠sticas de los
datos y aborde problemas como la heterocedasticidad y la no normalidad de los errores, as√≠
como cuando el rango de la variable de respuesta est√° limitado, entre otros.

En R, la funci√≥n glm() se utiliza para ajustar Modelos Lineales Generalizados. Esta
funci√≥n permite especificar tanto la distribuci√≥n de la variable dependiente como la
funci√≥n de enlace adecuada.

**Sintaxis b√°sica de glm()**

``` r
glm(formula, family = family_type(link = link_function), data = dataset)
```

-   `formula`: Especifica la relaci√≥n entre las variables dependiente e independientes
    (similar a `lm()`).

-   `family`: Define la distribuci√≥n de la variable dependiente. Puede ser `gaussian`
    (para regresi√≥n lineal), `binomial` (para regresi√≥n log√≠stica), `poisson` (para
    modelos de conteo), `gamma`, entre otros.

-   `link`: Es la funci√≥n de enlace que conecta la media de la variable dependiente con
    las variables independientes (por ejemplo, `log`, `identity` o `inverse`).

La funci√≥n de enlace predeterminada para una familia puede cambiarse especificando un
enlace a la funci√≥n de familia. Si no se informa nada, el modelo correr√° en la pr√°ctica un
modelo lineal simple. Comparen los resultados:

```{r}
glm(wage ~ educ + antig√ºedad + married, data = base1)
lm(wage ~ educ + antig√ºedad + married, data = base1)
```

Por ejemplo, si la variable de respuesta es no negativa y la varianza es proporcional a la
media, se usar√≠a la funci√≥n de enlace ‚Äúidentity‚Äù con la familia ‚Äúquasipoisson‚Äù. Esto se
especificar√≠a como:

``` r
family = quasipoisson(link = "identity")
```

La decisi√≥n sobre qu√© familia es apropiada no se discute a profundidad en esta secuencia,
pero estos pueden ser:

``` r
binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")
quasi(link = "identity", variance = "constant")
quasibinomial(link = "logit")
quasipoisson(link = "log")
```

-   gaussian: Para variables continuas que siguen una distribuci√≥n normal (equivalente a
    la regresi√≥n lineal cl√°sica).
-   binomial: Para variables categ√≥ricas binarias (regresi√≥n log√≠stica).
-   poisson: Para datos de conteo (n√∫meros enteros no negativos).
-   Gamma: Para variables continuas y positivas, especialmente cuando la varianza aumenta
    con la media.
-   inverse.gaussian: Para variables continuas positivas con varianza que aumenta
    r√°pidamente con la media.

Algunas funciones de enlace comunes son: - identity: Sin transformaci√≥n (usada en
regresi√≥n lineal). - log: Transforma la media mediante el logaritmo natural (√∫til para
variables positivas). - logit: Funci√≥n log√≠stica, usada en regresi√≥n log√≠stica. - inverse:
Utiliza la inversa de la media.

Cada familia tiene una funci√≥n de enlace predeterminada, pero puede modificarse seg√∫n las
necesidades del an√°lisis. Revisar la p√°gina de ayuda de `glm` y la documentacion de
objetos familiares para modelos ayudar√° en gran medida.

Volviendo al ejemplo que tra√≠amos antes, queremos modelar el salario (wage), que es una
variable continua y positiva, y sospechamos que la varianza aumenta con la media. Podemos
utilizar la familia Gamma con una funci√≥n de enlace logar√≠tmica, a trav√©s de
`family = Gamma(link = "log")`:

```{r}
# Ajustando un modelo GLM con distribuci√≥n gamma y enlace logar√≠tmico
mod_glm <- glm(wage ~ educ + antig√ºedad + married + female + nonwhite, 
                 family = Gamma(link = "log"), data = base1)

# Resumen del modelo
summary(mod_glm)
```

Los coeficientes representan el efecto multiplicativo de las variables independientes
sobre el salario. Por pasos:

El intercepto se interpreta como el valor cuando las variables independientes son cero. En
este caso, el logaritmo del salario esperado es 0.6336. - Exponenciando el intercepto:
$exp(0.6336) ‚âà 1.884$. - Esto significa que, para una persona con cero a√±os de educaci√≥n y
antig√ºedad, no casada, hombre y blanco, el salario promedio esperado es aproximadamente
\$1.88 por hora (recordemos, esto es de 1976).

Cada a√±o adicional de educaci√≥n se asocia con un incremento en el logaritmo del salario de
0.08177. Los coeficientes se han de exponenciar (y son multiplicativos, entendi√©ndose como
cambios porcentuales por unidad adicional).

```{r}
exp(coef(mod_glm))
```

Por ejemplo, por cada a√±o adicional de educaci√≥n, el salario promedio aumenta en
aproximadamente un 8.53%, manteniendo constantes las dem√°s variables. En el caso de la
variable 'dummy', como female, el exponenciado es menos que uno. Esto implica que las
mujeres ganaban, en promedio, un 24.82% menos que los hombres, manteniendo constantes las
dem√°s variables en el modelo. Otros elementos en las columnas del estimado son similares a
lo que vimos al usar `lm()`.

Sin embargo tenemos informaci√≥n adicional. Hay un par√°metro de dispersi√≥n. Este es un
estimado de la varianza de los residuos en el modelo Gamma. Un valor m√°s peque√±o indica
menor variabilidad de los datos alrededor del modelo ajustado. La ra√≠z cuadrada de ese
par√°metro es la desviaci√≥n est√°ndar estimada. Las desviaciones nulas y residuales son
comparaciones entre un modelo vac√≠o (con s√≥lo un intercepto), y el modelo ajustado con
todas las variables.

En este caso reporta el criterio de informaci√≥n de Akaike, que es una medida de la calidad
del modelo que penaliza por la complejidad (n√∫mero de par√°metros). Al comparar modelos, un
AIC m√°s bajo indica un modelo preferido. Como s√≥lo tenemos un modelo, el AIC nos sirve
para comparar con futuros modelos alternativos. Finalmente, declara la cantidad de
Iteraciones que fueron necesarias para converger y encontrar los estimadores de m√°xima
verosimilitud.

Verifiquemos otra vez con el modelo transformado

```{r}
# Resumen del modelo
summary(mod5)
```

Notamos que la transformaci√≥n aproxima los coeficientes a los conseguidos con el `glm()`.

```{r}
library(arm)

# Gr√°fico de coeficientes para el modelo GLM
coefplot(mod_glm, col.pts = "red", cex.pts = 1.5, main = "Comparaci√≥n de Coeficientes")

# A√±adimos los coeficientes del modelo OLS al mismo gr√°fico
coefplot(mod5, add = TRUE, col.pts = "blue", cex.pts = 1.5)

legend("topright", legend = c("GLM (Gamma con enlace log)", "OLS (log(wage))"), 
       col = c("red", "blue"), pch = 16)
```

## An√°lisis de la bondad del modelo

```{r}
# Generamos los gr√°ficos diagn√≥sticos
par(mfrow = c(2, 2))
plot(mod_glm)
```

Esto se ve posiblemente mejor que la variante de OLS, aunque tampoco parece perfecto.

# Modelos jer√°rquicos

Los modelos jer√°rquicos, tambi√©n conocidos como modelos lineales mixtos o modelos
multinivel, son una extensi√≥n de los modelos lineales que permiten analizar datos en los
que las observaciones est√°n agrupadas o anidadas en diferentes niveles. Estos modelos son
especialmente √∫tiles cuando se espera que exista correlaci√≥n entre las observaciones
dentro de los mismos grupos.

En este an√°lisis, utilizaremos el paquete `lme4` en R, que es ampliamente utilizado para
ajustar modelos lineales mixtos. A continuaci√≥n, te guiar√© a trav√©s de los pasos para
ajustar y comprender estos modelos, explicando cada componente y resultado de manera
clara. La sintaxis de `lme4` se basa en la sintaxis de los modelos lineales que ya
conocemos de `lm()`.

La funci√≥n `lmer()` de `lme4` a√±ade la especificaci√≥n de la variable de grupo/sujeto y de
la estructura de efectos aleatorios que se van a estimar. En par√©ntesis adicionales (ver
abajo), el t√©rmino a la izquierda de `|` especifica los efectos aleatorios que se van a
estimar. El t√©rmino a la derecha de `|` representa la(s) variable(s) que definen la
estructura de agrupaci√≥n (o anidamiento) de los datos.

Un `1` en la parte izquierda del par√©ntesis significa que se debe estimar un componente de
varianza de intercepto aleatorio. Si tambi√©n se coloca la variable predictora a la
izquierda de `|`, esto indica que se deben incluir pendientes aleatorias. La forma b√°sica
de estos ser√°:

``` r
lmer(data = datos, VarDependiente ~ VarIndependiente + (1 | VarGrupal))
```

Corresponde a un modelo que puede describirse de la siguiente manera: ‚ÄúLa variable
dependiente es predicha por la variable independiente. Al mismo tiempo, la varianza de los
residuos de nivel 2 del intercepto es un par√°metro del modelo‚Äù.

Comencemos con modelos HLM que incluyen solo variables predictoras de nivel 1 (individuos
anidados en grupos). Luego, en un segundo paso, a√±adiremos predictores de nivel 2.
Finalmente, analizaremos los modelos HLM de medidas repetidas, donde el nivel m√°s bajo
(nivel 1) corresponde a observaciones repetidas dentro de los individuos, y esas
observaciones est√°n anidadas en los participantes individuales (nivel 2).

Nos limitaremos aqu√≠ a modelos de 2 niveles. Los principios de los modelos HLM pueden
ilustrarse de manera bastante parsimoniosa de esta forma, y expandir los modelos a m√°s de
dos niveles de an√°lisis es bastante sencillo.

```{r}
# Cargar las librer√≠as necesarias
#library(tidyverse)
library(lme4)
library(lmerTest)

# Cargar los datos desde una URL
df <- read_csv("https://raw.githubusercontent.com/methodenlehre/data/master/salary-data.csv")

# Convertir las variables 'firma' y 'sector' en factores
df <- df |>
  mutate(firma = as.factor(firma),
         sector = as.factor(sector))

# Ver las √∫ltimas filas del conjunto de datos
tail(df)
```

## Modelo nulo

En este caso saco unos datos de salarios de compa√±√≠as ficticias en Suiza. Con `mutate()`
convert√≠ las variables firma (empresa) y sector en factores, ya que representan
categor√≠as. Antes de a√±adir predictores, ajustamos un modelo nulo que solo incluye el
intercepto y un t√©rmino aleatorio para capturar la variabilidad entre las empresas
(firma). Este modelo nos permite estimar la correlaci√≥n intraclase (ICC) y entender qu√©
proporci√≥n de la variabilidad total del salario se debe a diferencias entre empresas.

```{r}
library(Matrix)
library(lme4)
# Ajustar el modelo nulo con intercepto aleatorio por empresa
modelo_nulo <- lmer(salary ~ 1 + (1 | firma), data = df, REML = TRUE)

# Obtener las predicciones del modelo nulo
df$predicciones_nulo <- predict(modelo_nulo)

# Resumen del modelo
summary(modelo_nulo)
```

Explicaci√≥n:

-   lmer(): Ajusta un modelo lineal mixto.
-   `salary ~ 1`: Indica que solo se ajusta el intercepto fijo (media general del
    salario).
-   `(1 | firma)`: Especifica un intercepto aleatorio para cada empresa (firma),
    capturando la variabilidad entre empresas.
-   `REML = TRUE`: Utiliza el m√©todo de M√°xima Verosimilitud Restringida para estimar los
    par√°metros, lo cual es apropiado cuando se comparan modelos con diferentes efectos
    fijos.
-   `predict()`: Genera predicciones del modelo para cada observaci√≥n.

La varianza del intercepto aleatorio (firma) representa la variabilidad del salario
promedio entre empresas. Por otro lado la varianza residual captura la variabilidad del
salario dentro de las empresas. Calcularemos la correlaci√≥n intraclase como la proporci√≥n
de la varianza total que se debe a diferencias entre empresas. Se define como:
$\rho = \frac{\sigma^2_{\text{Nivel-2}}}{\sigma^2_{\text{Nivel-2}} + \sigma^2_{\text{Nivel-1}}}$

```{r}
# Extraer las varianzas del modelo
var_intercepto <- as.numeric(VarCorr(modelo_nulo)$firma[1])
var_residual <- attr(VarCorr(modelo_nulo), "sc")^2

# Calcular la ICC
ICC <- var_intercepto / (var_intercepto + var_residual)
ICC
```

Correlaci√≥n intraclase:
$\hat{\rho} = \frac{\hat{\sigma}^2_{œÖ_0}}{\hat{\sigma}^2_{œÖ_0} + \hat{\sigma}^2_{œµ}} = \frac{851249}{851249 + 1954745} = 0.3034$

Es decir, 30% de la variabilidad total del salario se debe a diferencias entre empresas.
Existe una correlaci√≥n notable entre los salarios de los empleados dentro de la misma
empresa, lo que justifica el uso de un modelo multinivel.

Con la funci√≥n ranef() podemos ver los efectos aleatorios (residuos de segundo nivel del
intercepto):

```{r}
ranef(modelo_nulo)
```

Para determinar si la varianza entre empresas es estad√≠sticamente significativa,
comparamos el modelo nulo con un modelo sin efectos aleatorios (modelo lineal simple).

```{r}
# Utilizar la funci√≥n ranova() para comparar modelos
anova_aleatorio <- ranova(modelo_nulo)
anova_aleatorio
```

Este compar√≥ el modelo actual con un modelo reducido que elimina el efecto aleatorio. El
valor p indica que incluir el intercepto aleatorio para firma mejora significativamente el
modelo. Aproximadamente, el 30% de la varianza total del salario se puede atribuir a
diferencias entre empresas.

## Modelo con predictor de primer nivel

Ahora, a√±adimos un predictor de nivel 1 (experience), que es una variable individual, al
modelo. Mantenemos el intercepto aleatorio para capturar la variabilidad entre empresas.

```{r}
# Ajustar el modelo con 'experience' como predictor fijo y intercepto aleatorio por empresa
modelo_intercepto_aleatorio <- lmer(salary ~ experience + (1 | firma), data = df, REML = TRUE)

# Obtener las predicciones del modelo
df$predicciones_intercepto_aleatorio <- predict(modelo_intercepto_aleatorio)

# Resumen del modelo
summary(modelo_intercepto_aleatorio)
```

Los resultados del modelo mixto muestran que el efecto fijo de la experiencia es
significativamente positivo, con un coeficiente estimado de $\hat{\gamma}_{10} = 534.34$ y
un valor $p$ menor a 0.001. Esto indica que, en promedio, el salario aumenta en
aproximadamente 534 unidades (por ejemplo, francos suizos) por cada a√±o adicional de
experiencia, controlando por las diferencias entre empresas. El modelo incluye un
intercepto aleatorio a nivel de empresa (firma), lo que nos permite ajustar diferencias
entre las empresas en los salarios base.

Se le puede a√±adir un nivel adicional al modelo para tanto acomodar por un intercepto as√≠
como pendiente aleatoria. Al aplicar este modelo la varianza (aleatoria) de la pendiente
de la variable independiente de primer nivel se puede estimar, d√°ndonos una idea de las
diferencias en el efecto (pendiente) de experiencia en salario entre las unidades del
segundo nivel (firmas). Si comparamos los efectos entre este y el modelo anterior, vemos
la disminuci√≥n marginal de la varianza del intercepto aleatorio en comparaci√≥n con el
modelo nulo, lo que sugiere que parte de la variabilidad entre empresas se explica por la
experiencia de los empleados.

```{r}
fixef(modelo_intercepto_aleatorio)
ranef(modelo_intercepto_aleatorio)
```

## Modelo con pendientes aleatorias

Para investigar si el efecto de la experiencia en el salario var√≠a entre empresas,
ajustamos un modelo que incluye tanto intercepto como pendiente aleatorios para
experience.

```{r}
# Ajustar el modelo con intercepto y pendiente aleatorios por empresa
modelo_coeficientes_aleatorios <- lmer(salary ~ experience + (experience | firma), data = df, REML = TRUE)

# Obtener las predicciones del modelo
df$predicciones_coeficientes_aleatorios <- predict(modelo_coeficientes_aleatorios)

# Resumen del modelo
summary(modelo_coeficientes_aleatorios)
```

En este caso `(experience | firma)` especifica que tanto el intercepto como la pendiente
de experience var√≠an aleatoriamente entre empresas.

Los resultados muestran que el efecto fijo de la experiencia sigue siendo
significativamente positivo, con un coeficiente estimado de $\hat{\gamma}_{10} = 530.85$ y
un valor $p$ menor a 0.001. Esto indica que, en promedio, el salario aumenta en
aproximadamente 531 unidades (por ejemplo, CHF para estos datos ficticios) por cada a√±o
adicional de experiencia. Una covarianza negativa sugiere que las empresas con salarios
base m√°s altos tienden a tener incrementos m√°s peque√±os por a√±o de experiencia, y
viceversa.

Para evaluar si el modelo con pendientes aleatorias proporciona un mejor ajuste que el
modelo con solo intercepto aleatorio, realizamos una comparaci√≥n de modelos.

```{r}
# Ajustar el modelo con intercepto aleatorio (modelo reducido)
modelo_intercepto_aleatorio <- lmer(salary ~ experience + (1 | firma), data = df, REML = FALSE)

# Ajustar el modelo con intercepto y pendiente aleatorios (modelo completo)
modelo_coeficientes_aleatorios <- lmer(salary ~ experience + (experience | firma), data = df, REML = FALSE)

# Comparar los modelos utilizando ANOVA
anova(modelo_intercepto_aleatorio, modelo_coeficientes_aleatorios)
```

El p-valor peque√±o indica que el modelo con pendientes aleatorias es significativamente
mejor que el modelo con solo intercepto aleatorio. Existe variaci√≥n significativa en el
efecto de la experiencia en el salario entre empresas.

Para ayudar a comprender mejor los resultados, es √∫til visualizar c√≥mo var√≠a el efecto de
la experiencia en el salario entre empresas.

```{r}
#library(ggplot2)
# Extraer los coeficientes aleatorios
coeficientes_aleatorios <- coef(modelo_coeficientes_aleatorios)$firma

# Renombrar las columnas
colnames(coeficientes_aleatorios) <- c("Intercepto", "Pendiente")

# A√±adir el identificador de la empresa
coeficientes_aleatorios$firma <- rownames(coeficientes_aleatorios)

# Graficar las pendientes de experiencia por empresa
ggplot(coeficientes_aleatorios, aes(x = reorder(firma, Pendiente), y = Pendiente)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Pendientes de Experiencia por Empresa",
       x = "Empresa",
       y = "Efecto de la Experiencia en el Salario") +
  theme_minimal()

# Crear un conjunto de datos con las predicciones del modelo
df$predicciones <- predict(modelo_coeficientes_aleatorios)

# Graficar los datos y las l√≠neas de regresi√≥n por empresa
ggplot(df, aes(x = experience, y = salary, color = firma, group = firma)) +
  geom_point(alpha = 0.5) +
  geom_line(aes(y = predicciones)) +
  labs(title = "Relaci√≥n entre experiencia y salario por empresa",
       x = "Experiencia",
       y = "Salario") +
  theme_minimal()
```

# Modelos longitudinales

Los modelos longitudinales son una herramienta estad√≠stica esencial para analizar datos en
los que se realizan observaciones repetidas de los mismos individuos a lo largo del
tiempo. Este tipo de an√°lisis es fundamental en campos como la medicina, psicolog√≠a,
econom√≠a, pol√≠tica, sociolog√≠a y otras disciplinas donde es importante entender c√≥mo
cambian las mediciones en los individuos o unidades a lo largo del tiempo y qu√© factores
influyen en esos cambios.

Los datos longitudinales son un caso especial de datos jer√°rquicos o multinivel, donde las
observaciones (mediciones) est√°n anidadas dentro de individuos. En este contexto, las
observaciones repetidas de un mismo individuo tienden a ser m√°s similares entre s√≠ que las
observaciones de diferentes individuos. Esta correlaci√≥n dentro de los sujetos es crucial
y debe ser tenida en cuenta para obtener inferencias estad√≠sticas v√°lidas. Una vista
general sobre paquetes √∫tiles en R para analizar este tipo de datos correlacionados se
puede encontrar en la [CRAN Task View
dedicada](https://cran.r-project.org/web/views/MixedModels.html).

En este taller, exploraremos c√≥mo ajustar y analizar modelos longitudinales utilizando R,
centr√°ndonos en un conjunto de datos que mide el crecimiento dental en ni√±os y ni√±as a
diferentes edades. Trabajaremos con un conjunto de datos que contiene medidas
longitudinales del crecimiento dental en 27 ni√±os (16 ni√±os y 11 ni√±as). Las medidas
corresponden a la distancia pituitaria-pterigomaxilar (una medida de crecimiento dental) y
se tomaron a las edades de 8, 10, 12 y 14 a√±os. Nuestro objetivo es describir c√≥mo cambia
esta distancia con la edad y comparar el patr√≥n de crecimiento entre ni√±os y ni√±as.

Primero, cargamos los datos y observamos su estructura:

```{r}
load(url("http://alecri.github.io/downloads/data/dental.RData"))
head(dental)
```

Los datos se presentan en formato ancho, donde las mediciones repetidas se encuentran en
columnas separadas para cada edad. Este formato no es ideal para el an√°lisis longitudinal,
por lo que convertiremos los datos a un formato largo usando la funci√≥n pivot_longer():

```{r}
library(labelled)   # etiquetado de datos
library(rstatix)    # estad√≠sticas descriptivas
library(ggpubr)     # estad√≠sticas descriptivas y gr√°ficos convenientes
library(GGally)     # gr√°ficos avanzados
library(car)        # √∫til para ANOVA/pruebas de Wald
library(Epi)        # f√°cil obtenci√≥n de intervalos de confianza para coeficientes/predicciones del modelo
#library(lme4)       # modelos lineales de efectos mixtos
#library(lmerTest)   # pruebas para modelos lineales de efectos mixtos
library(emmeans)    # medias marginales
library(multcomp)   # intervalos de confianza para combinaciones lineales de coeficientes del modelo
library(geepack)    # ecuaciones de estimaci√≥n generalizadas
library(ggeffects)  # efectos marginales, predicciones ajustadas
library(gt)         # tablas bonitas
dental_long <- pivot_longer(dental, cols = starts_with("y"), 
                            names_to = "measurement", values_to = "distance") |>
  mutate(
    age = parse_number(measurement),
    measurement = fct_inorder(paste("Medida a los", age))
  ) |>
  set_variable_labels(
    age = "Edad del ni√±o/a al momento de la medici√≥n",
    measurement = "Etiqueta de medici√≥n temporal",
    distance = "Medici√≥n de distancia"
  )

head(dental_long)
```

Ahora, cada fila representa una medici√≥n de un individuo en una edad espec√≠fica, lo que
facilita el an√°lisis longitudinal. Antes de ajustar modelos, exploramos los datos de forma
descriptiva para entender las tendencias generales.

```{r}
group_by(dental_long, age) |> 
  get_summary_stats(distance)
```

Observamos que la distancia media aumenta con la edad, lo que sugiere un crecimiento
dental a medida que los ni√±os y ni√±as crecen. Creamos un diagrama de caja para visualizar
la distribuci√≥n de la distancia a cada edad:

```{r}
ggplot(dental_long, aes(measurement, distance, fill = measurement)) +
  geom_boxplot() +
  geom_jitter(width = 0.2) +
  guides(fill = "none") +
  labs(x = "Edad (a√±os)", y = "Crecimiento dental, mm")
```

El gr√°fico muestra que la mediana y los valores de la distancia aumentan con la edad. La
dispersi√≥n tambi√©n parece aumentar ligeramente, lo que indica mayor variabilidad en edades
superiores. Exploremos si hay diferencias en el crecimiento dental entre ni√±os y ni√±as.

```{r}
group_by(dental_long, sex, measurement) |> 
  get_summary_stats(distance, show = c("mean", "sd"))
```

Y gr√°ficamente

```{r}
ggplot(dental_long, aes(sex, distance, fill = measurement)) +
  geom_boxplot() +
  labs(x = "", y = "Crecimiento dental, mm", fill = "")

```

Visualmente, parece que los ni√±os tienen, en promedio, una distancia ligeramente mayor que
las ni√±as en cada edad.

Evaluemos las correlaciones entre medidas de inter√©s:

```{r}
ggpairs(select(dental, starts_with("y")), lower = list(continuous = "smooth"))
```

y separemos nuevamente por grupo (sexo)

```{r}
ggpairs(dental, mapping = aes(colour = sex,alpha = 0.5), columns = 3:6,
        lower = list(continuous = "smooth"))
```

Digamos que queremos entender si el efecto var√≠a con el tiempo. Podr√≠amos evaluar
gr√°ficamente esto:

```{r}
group_by(dental_long, sex, age) |> 
  summarise(mean = list(mean_ci(distance)), .groups = "drop") |> 
  unnest_wider(mean) |> 
  mutate(agex = age - .05 + .05*(sex == "Boy")) |> 
  ggplot(aes(agex, y, col = sex, shape = sex)) +
  geom_point() +
  geom_errorbar(aes(ymin = ymin, ymax = ymax), width = 0.2) +
  geom_line() +
  labs(x = "Edad, en a√±os", y = "Crecimiento dental medio, mm", shape = "Sex", col = "Sex")

ggplot(dental_long, aes(x = age, y = distance, color = sex)) +
  geom_point() +
  geom_line(aes(group = id)) +
  labs(x = "Edad (a√±os)", y = "Crecimiento dental (mm)", color = "Sexo") +
  theme_minimal()
```

Para analizar los datos longitudinales, utilizamos modelos mixtos lineales, que permiten
modelar tanto los efectos fijos (comunes a todos los individuos) como los efectos
aleatorios (espec√≠ficos de cada individuo).

## Modelo inicial

Ajustamos un modelo que considera la edad como factor categ√≥rico (variables indicadoras) y
un intercepto aleatorio para capturar la variabilidad entre individuos.

```{r}

# Ajustar el modelo mixto
modelo_edad <- lmer(distance ~ factor(age) + (1 | id), data = dental_long)

# Resumen del modelo
summary(modelo_edad)
```

En este caso el intercepto captura la distancia media dental a los 8 a√±os (valor de
referencia). Esta es 22.18 mm. La diferencia entre las respuestas medias de los ni√±os de
10 y 8 a√±os es de 0.98 mm. La diferencia entre las respuestas medias de los ni√±os de 12 y
8 a√±os es de 2.46 mm. La diferencia entre las respuestas medias de los ni√±os de 14 y 8
a√±os es de 3.91 mm.

Esto indica que la distancia pituitaria-pterigomaxilar aumenta significativamente con la
edad, lo que es consistente con el crecimiento observado durante este per√≠odo.

Usamos un an√°lisis de varianza para evaluar si la edad tiene un efecto significativo en el
crecimiento dental.

```{r}
library(car)

Anova(modelo_edad)
```

El efecto de la edad es altamente significativo (p \< 0.001), indicando que la edad
influye en el crecimiento dental.

Calculamos las medias marginales estimadas para cada edad y sus intervalos de confianza.

```{r}
library(emmeans)

# Calcular las medias marginales
medias_edad <- emmeans(modelo_edad, ~ age)

# Mostrar los resultados
summary(medias_edad)
```

Las medias estimadas confirman que la distancia media aumenta con la edad, aumentando la
tasa de crecimiento en cada momento de medici√≥n. Podemos presentar la trayectoria estimada
del modelo ajustado utilizando la funci√≥n predict:

```{r}
# Generar predicciones del modelo ajustado
predicciones <- predict(modelo_edad)

# Visualizar la trayectoria estimada
ggplot(dental_long, aes(x = age, y = distance, color = sex)) +
  geom_point() +
  geom_line(aes(y = predicciones, group = id), linetype = "dashed") +
  labs(x = "Edad (a√±os)", y = "Distancia pituitaria-pterigomaxilar (mm)", color = "Sexo") +
  theme_minimal()
```

Este gr√°fico muestra tanto los datos reales de distancia en funci√≥n de la edad como las
predicciones del modelo para cada ni√±o, con l√≠neas discontinuas que representan las
trayectorias estimadas a lo largo del tiempo.

## Modelo con variable de sexo

```{r}
# Ajustar el modelo con sexo y su interacci√≥n con la edad
modelo_sexo <- lmer(distance ~ factor(age) * sex + (1 | id), data = dental_long)

# Resumen del modelo
summary(modelo_sexo)
```

Interpretando salidas:

-   La varianza del Intercepto Aleatorio (id) es 3.285 con una desviaci√≥n est√°ndar de
    1.813. Existe variabilidad significativa entre los individuos en sus medidas iniciales
    de distancia dental.

-   La varianza residual es 1.975 con una desviaci√≥n est√°ndar de 1.405. Esta captura
    variabilidad no explicadas por el modelo.

-   El intercepto captura el ser una ni√±a de 8 a√±os como el punto de referencia:
    21.1818mm.

-   Coeficientes de factor(g)AA: El efecto de edad est√° d√°ndonos el cambio en la distancia
    dental en comparaci√≥n con la edad de referencia (8 a√±os) para las ni√±as. Por ejemplo,
    las ni√±as de 10 a√±os tienen, en promedio, una distancia dental 1.05 mm mayor que las
    ni√±as de 8 a√±os, mientras que las de 14 a√±os tienen, en promedio, una distancia dental
    2.91 mm mayor que las ni√±as de 8 a√±os.

-   Coeficiente `sexBoy`: Los ni√±os de 8 a√±os tienen, en promedio, una distancia dental
    1.69 mm mayor que las ni√±as de 8 a√±os. Esta es una variable dummy.

-   Los coeficientes de interacci√≥n indican cu√°nto cambia la diferencia entre ni√±os y
    ni√±as en cada edad en comparaci√≥n con la edad de referencia (8 a√±os). Por ejemplo, a
    los 14 a√±os, los ni√±os tienen una distancia dental adicional de 1.68 mm en comparaci√≥n
    con las ni√±as, m√°s all√° de la diferencia observada a los 8 a√±os. Esa edad es la √∫nica
    que reporta distancias significativas a la referencia; al entrar m√°s en la
    adolescencia las diferencias se tornan notables.

```{r}
Anova(modelo_sexo)
```

El efecto de la edad es altamente significativo. Esto indica que existen diferencias
significativas en la distancia dental media entre las diferentes edades (8, 10, 12 y 14
a√±os), independientemente del sexo. Tambi√©n el sexo del paciente parece ser significativo.
La interacci√≥n entre edad y sexo es marginalmente significativa (p \< 0.10, pero \> 0.05).

# Series de tiempo: ejemplo de finanzas

Las series de tiempo son un conjunto de observaciones registradas en momentos sucesivos en
el tiempo, ordenadas cronol√≥gicamente. El an√°lisis de series de tiempo es fundamental en
diversos campos como la econom√≠a, finanzas, meteorolog√≠a, y ciencias sociales, ya que
permite comprender y predecir comportamientos futuros basados en datos hist√≥ricos.

En esta √±apita del taller, me enfoco en el an√°lisis de series de tiempo financieras,
espec√≠ficamente en el estudio de los precios de las acciones. El objetivo es proporcionar
una gu√≠a clara y pr√°ctica sobre modelos de series de tiempo utilizando R.

Las series de tiempo financieras, como los precios de las acciones o los tipos de cambio,
son intr√≠nsecamente vol√°tiles y est√°n influenciadas por m√∫ltiples factores econ√≥micos y
pol√≠ticos. El an√°lisis de estas series permite a los inversores y analistas comprender las
tendencias del mercado, evaluar riesgos, anticiparse a estas y tomar decisiones
informadas.

Para este an√°lisis, utilizaremos datos hist√≥ricos del precio de cierre ajustado de las
acciones de Apple Inc. (AAPL). Usaremos el paquete quantmod para descargar los datos
directamente desde Yahoo Finance.

```{r}
# Instalar paquetes si no est√°n instalados
#install.packages("quantmod")
#install.packages("forecast")
#install.packages("tseries")
#install.packages("ggplot2")
#install.packages("quantmod", repos = "https://cloud.r-project.org/")

# Cargar los paquetes
library(quantmod)
library(forecast)
library(tseries)
library(ggplot2)
```

Descargamos los datos de los √∫ltimos cinco a√±os.

```{r}
# Establecer el rango de fechas
fecha_inicio <- as.Date("2019-01-01")
fecha_fin <- as.Date(Sys.Date())

# Descargar los datos de AAPL
getSymbols("AAPL", src = "yahoo", from = fecha_inicio, to = fecha_fin)

# Ver las primeras filas
head(AAPL)
```

Extraemos la columna de precios de cierre ajustados y creamos un objeto de serie de
tiempo.

```{r}
# Extraer el precio de cierre ajustado
precio_cierre <- AAPL[, "AAPL.Adjusted"]

# Convertir a serie de tiempo
serie_tiempo <- ts(precio_cierre, frequency = 252)  # 252 d√≠as h√°biles en un a√±o
```

Antes de ajustar cualquier modelo, es importante entender las caracter√≠sticas de la serie
de tiempo.

```{r}
# Graficar la serie de tiempo
autoplot(serie_tiempo) +
  labs(title = "Precio de Cierre Ajustado de AAPL",
       x = "Tiempo",
       y = "Precio ($)") +
  theme_minimal()
```

El gr√°fico muestra la evoluci√≥n del precio de AAPL desde 2019 hasta la fecha actual.
Observamos tendencias ascendentes y periodos de volatilidad, especialmente durante eventos
econ√≥micos significativos como la pandemia de COVID-19 y la subsiguiente alza en
valuaciones financieras.

Descomponemos la serie para analizar sus componentes: tendencia, estacionalidad y
residuales.

```{r}
# Verificar la estructura de 'serie_tiempo'
str(serie_tiempo)
# Convertir a vector num√©rico
precio_cierre_vector <- as.numeric(precio_cierre)

# Crear la serie de tiempo
serie_tiempo <- ts(precio_cierre_vector, frequency = 252)

# Descomposici√≥n utilizando STL (Seasonal and Trend decomposition using Loess)
descomposici√≥n <- stl(serie_tiempo, s.window = "periodic")

# Graficar la descomposici√≥n
autoplot(descomposici√≥n) +
  labs(title = "Descomposici√≥n de la Serie de Tiempo de AAPL")
```

-   Tendencia: Muestra el movimiento a largo plazo del precio.
-   Estacionalidad: En series financieras diarias, la estacionalidad puede no ser
    pronunciada, pero pueden existir patrones semanales o mensuales.
-   Residuales: Parte de la serie no explicada por la tendencia ni la estacionalidad.

La estacionariedad es una propiedad clave en el an√°lisis de series de tiempo. Una serie
estacionaria tiene estad√≠sticas (media, varianza) constantes en el tiempo. Realizamos la
prueba Dickey-Fuller Aumentada (ADF) para verificar si la serie es estacionaria.

```{r}
# Prueba ADF
adf.test(serie_tiempo, alternative = "stationary")
```

El p-valor es alto (mayor que 0.05), lo que indica que no podemos rechazar la hip√≥tesis
nula de que la serie tiene una ra√≠z unitaria (no estacionaria). Concluimos que la serie no
es estacionaria.

Para lograr estacionariedad, transformamos los precios en rendimientos logar√≠tmicos.

```{r}
# Calcular los rendimientos logar√≠tmicos
rendimientos <- diff(log(serie_tiempo))

# Graficar los rendimientos
autoplot(rendimientos) +
  labs(title = "Rendimientos Logar√≠tmicos Diarios de AAPL",
       x = "Tiempo",
       y = "Rendimiento") +
  theme_minimal()
```

Verificamos nuevamente la serie transformada con ADF.

```{r}
# Prueba ADF en los rendimientos
adf.test(rendimientos, alternative = "stationary")
```

El p-valor es bajo (0.01), lo que indica que podemos rechazar la hip√≥tesis nula.

El modelo ARIMA (Autoregressive Integrated Moving Average) es ampliamente utilizado para
modelar series de tiempo estacionarias. Analizamos las funciones de autocorrelaci√≥n (ACF)
y autocorrelaci√≥n parcial (PACF) para identificar los √≥rdenes del modelo.

```{r}
# Graficar ACF y PACF
ggAcf(rendimientos, lag.max = 20) + ggtitle("Funci√≥n de Autocorrelaci√≥n (ACF)")
ggPacf(rendimientos, lag.max = 20) + ggtitle("Funci√≥n de Autocorrelaci√≥n Parcial (PACF)")
```

Las gr√°ficas nos ayudan a identificar posibles valores de p y q para el modelo ARIMA(p, d,
q). En los rendimientos financieros, a menudo se observa poca autocorrelaci√≥n
significativa. Tambi√©n podr√≠amos hacer que el algoritmo seleccione los valores para el
modelo. Para esto utilizamos la funci√≥n `auto.arima()`, y seleccionar√° estos usando
criterios de informaci√≥n (AICc).

```{r}
# Selecci√≥n autom√°tica del modelo ARIMA
modelo_arima <- auto.arima(rendimientos, seasonal = FALSE)

# Resumen del modelo
summary(modelo_arima)
```

El modelo seleccionado es un ARIMA(1,0,0), es decir, un modelo ARMA con un t√©rmino
autorregresivo pero sin media m√≥vil, realmente, un modelo AR.

Verificamos si el modelo ajustado cumple con los supuestos necesarios.

```{r}
# Graficar los residuos
checkresiduals(modelo_arima)
```

-   Residuos Estandarizados: Deben comportarse como ruido blanco (‚úÖ).
-   ACF de Residuos: No debe mostrar autocorrelaci√≥n significativa (‚úÖ)..
-   Prueba de Ljung-Box: Un p-valor alto indica que no hay autocorrelaci√≥n en los residuos
    (üíÖ).

Podr√≠amos verificar la heteroscedasticidad. En series financieras, es com√∫n que exista
esta estructura en los datos.

```{r}
# Prueba de Arch
library(FinTS)
ArchTest(resid(modelo_arima))
```

La prueba es significativa,

Para capturar la heterocedasticidad, utilizamos modelos GARCH (Generalized Autoregressive
Conditional Heteroskedasticity). Utilizamos el paquete `rugarch` para ajustar un modelo
GARCH.

```{r}
# Instalar y cargar el paquete rugarch
#install.packages("rugarch")
library(rugarch)

# Especificar el modelo ARIMA(1,0,0)-GARCH(1,1)
especificacion <- ugarchspec(mean.model = list(armaOrder = c(1,0)),
                             variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
                             distribution.model = "norm")

# Ajustar el modelo
modelo_garch <- ugarchfit(spec = especificacion, data = rendimientos)

# Resumen del modelo
show(modelo_garch)
```

-   Mu ($Œº$): El t√©rmino de la media es significativo ($p < 0.01$), lo que indica que hay
    un rendimiento promedio distinto de cero.
-   AR(1): El coeficiente AR(1) no es significativo ($p ‚âà 0.42$), lo que sugiere que el
    t√©rmino autorregresivo puede no ser necesario.
-   Omega ($œâ$): Par√°metro significativo que representa la varianza incondicional.
-   Alpha1 ($Œ±‚ÇÅ$) y Beta1 ($Œ≤‚ÇÅ$): Ambos son altamente significativos, confirmando la
    presencia de efectos ARCH y GARCH.

Las pruebas diagn√≥sticas indican que no aparentan haber ni autocorrelaci√≥n significativa
en los residuos ni en los residuos al cuadrado, as√≠ como tampoco aparentan haber sesgos
significativos en los residuos (signos). El modelo GARCH parece haber capturado
adecuadamente la heteroscedasticidad. Los par√°metros podr√≠an ser algo inestables
(verificar Nyblom)

```{r}
# Graficar los residuos estandarizados
plot(modelo_garch, which = "all")
```

Podr√≠a considerar un modelo m√°s sencillo sin AR(1), y con colas m√°s amplias en
distribuci√≥n t.

```{r}
# Especificaci√≥n de un modelo GARCH(1,1) sin t√©rmino AR(1)
especificacion_simple <- ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                    variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
                                    distribution.model = "norm")

modelo_garch_simple <- ugarchfit(spec = especificacion_simple, data = rendimientos)

# Modelo GARCH con distribuci√≥n t de Student
especificacion_t <- ugarchspec(mean.model = list(armaOrder = c(0,0)),
                               variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
                               distribution.model = "std")

modelo_garch_t <- ugarchfit(spec = especificacion_t, data = rendimientos)
modelo_garch_t
```

Finalmente, quiero visualizar usando el ARIMA(1,0,0) el rendimiento futuro de AAPL por 15
d√≠as.

Uso para esto la funci√≥n `forecast()`: Esta funci√≥n del paquete forecast genera
pron√≥sticos basados en el modelo ARIMA ajustado (`modelo_arima`). Con `h=15` indico que
quiero saber los pr√≥ximos 15 periodos (d√≠as). Graficamos ese pron√≥stico con `autoplot`, y
algo de personalizaci√≥n.

```{r}
# Pron√≥stico de los pr√≥ximos 15 d√≠as
pronostico_arima <- forecast(modelo_arima, h = 15)

# Graficar el pron√≥stico
autoplot(pronostico_arima) +
  labs(title = "Pron√≥stico ARIMA de los Rendimientos de AAPL",
       x = "Tiempo",
       y = "Rendimiento") +
  theme_minimal()
```

```{r}
# Pron√≥stico de la volatilidad futura
pronostico_garch <- ugarchforecast(modelo_garch, n.ahead = 10)

# Extraer la volatilidad pronosticada
volatilidad_pronosticada <- sigma(pronostico_garch)

# Mostrar los resultados
volatilidad_pronosticada
```

-   Los pron√≥sticos nos ayudan a entender las posibles tendencias futuras en los
    rendimientos y la volatilidad.

Conclusiones

-   An√°lisis Exploratorio: Identificamos que los precios de las acciones no son
    estacionarios, pero los rendimientos logar√≠tmicos s√≠ lo son.
-   Modelado ARIMA: Ajustamos un modelo ARIMA para capturar la din√°mica en los
    rendimientos.
-   Volatilidad y GARCH: Detectamos heterocedasticidad y utilizamos un modelo GARCH para
    modelar la volatilidad condicional.
-   Pron√≥sticos: Generamos pron√≥sticos de rendimientos y volatilidad.

Es importante tener en cuenta que los mercados financieros son influenciados por muchos
factores impredecibles, y ning√∫n modelo puede capturar completamente esa complejidad. Por
lo tanto, los pron√≥sticos deben interpretarse con cautela y complementarse con an√°lisis
cualitativos y contextualizados.

Recomiendo aplicar estos m√©todos a diferentes activos financieros o √≠ndices para
fortalecer la comprensi√≥n. Adem√°s de R, existen otras herramientas como Python y EViews
que tambi√©n son √∫tiles para el an√°lisis de series de tiempo. EViews est√° en declive seg√∫n
tengo entendido, pero sigue siendo utilizado en varios espacios.

# Qu√© aprendimos en este taller

Aprendimos la existencia de varios modelos, maneras de representar gr√°ficamente sus
resultados, y evaluar su utilidad. He a√±adido elementos que aumentan la descripci√≥n de
pedazos del taller, y que buscan corregir el orden en el que se trabajaron ejemplos.
